# -*- coding: utf-8 -*-
"""Brain_Deep_Lap

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VQifuZfGZ4cypR5Vz08B1YrDU4heTYXt
"""

from google.colab import files
print("Upload your kaggle.json file (download from Kaggle > Account > Create New API Token)")
files.upload()
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Part 1: Setup and Data Loading for Brain Segmentation with DeepLab
# Run this in Google Colab

import os
import sys
import urllib.request
import tarfile
import zipfile
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import nibabel as nib
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

print("üöÄ Part 1: Setting up Brain Segmentation with DeepLab")
print("="*60)

# ============================================================================
# STEP 1: Install Required Packages
# ============================================================================

def install_requirements():
    """Install all required packages for the project"""

    packages = [
        'nibabel',                    # For medical image processing
        'scikit-image',              # Image processing
        'opencv-python',             # Computer vision
        'plotly',                    # 3D visualization
        'ipywidgets',                # Interactive widgets
        'torch torchvision',         # PyTorch
        'torchvision',               # DeepLab models
        'monai',                     # Medical imaging AI
        'vtk',                       # 3D visualization
        'pyvista',                   # 3D mesh processing
        'trimesh',                   # 3D geometry
        'pillow',                    # Image handling
        'scipy'                      # Scientific computing
    ]

    print("üì¶ Installing required packages...")
    for package in packages:
        try:
            os.system(f'pip install -q {package}')
            print(f"‚úÖ Installed: {package}")
        except:
            print(f"‚ö†Ô∏è Failed to install: {package}")

# Uncomment the next line to install packages
# install_requirements()

# ============================================================================
# STEP 2: Setup Directories
# ============================================================================

def setup_directories():
    """Create project directory structure"""

    base_dir = Path("/content/brain_segmentation")
    dirs = {
        'base': base_dir,
        'data': base_dir / "data",
        'models': base_dir / "models",
        'results': base_dir / "results",
        'visualizations': base_dir / "visualizations",
        'checkpoints': base_dir / "checkpoints"
    }

    for name, path in dirs.items():
        path.mkdir(parents=True, exist_ok=True)
        print(f"‚úÖ Created: {path}")

    return dirs

# Setup directories
directories = setup_directories()

# ============================================================================
# STEP 3: Download and Extract Dataset
# ============================================================================

def download_dataset(data_dir):
    """Download MRBrainS13 dataset"""

    url = "https://dataverse.nl/api/access/dataset/:persistentId/?persistentId=doi:10.34894/645ZIN"
    download_path = data_dir / "MRBrainS13_download"

    print("üì• Downloading brain dataset...")
    try:
        # Download the file
        urllib.request.urlretrieve(url, download_path)
        print(f"‚úÖ Downloaded to: {download_path}")

        # Check the file type by reading first few bytes
        with open(download_path, 'rb') as f:
            header = f.read(512)

        # Determine file type and rename accordingly
        if header.startswith(b'PK'):
            # ZIP file
            final_path = data_dir / "MRBrainS13.zip"
            download_path.rename(final_path)
            print(f"üì¶ Detected: ZIP archive")
            return final_path
        elif header[257:262] == b'ustar':
            # TAR file
            final_path = data_dir / "MRBrainS13.tar"
            download_path.rename(final_path)
            print(f"üì¶ Detected: TAR archive")
            return final_path
        elif header.startswith(b'\x1f\x8b'):
            # GZIP file
            final_path = data_dir / "MRBrainS13.tar.gz"
            download_path.rename(final_path)
            print(f"üì¶ Detected: GZIP archive")
            return final_path
        else:
            # Unknown format - might be HTML or error page
            print(f"‚ö†Ô∏è Unknown file format. First 100 bytes:")
            print(header[:100])
            return None

    except Exception as e:
        print(f"‚ùå Download failed: {e}")
        return None

def extract_dataset(archive_path, data_dir):
    """Extract the dataset"""

    if not archive_path or not archive_path.exists():
        return None

    print("üìÇ Extracting dataset...")
    try:
        # Determine extraction method based on file extension
        if archive_path.suffix == '.zip':
            with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                zip_ref.extractall(data_dir)
            # Check if there are nested zip files and extract them
            for item in data_dir.iterdir():
                if item.is_file() and item.suffix == '.zip' and item.name != archive_path.name:
                    print(f"  Nested ZIP found: {item.name}. Extracting...")
                    with zipfile.ZipFile(item, 'r') as nested_zip_ref:
                        nested_zip_ref.extractall(data_dir)
                    item.unlink() # Remove the nested zip file after extraction
        elif archive_path.suffix == '.gz':
            with tarfile.open(archive_path, 'r:gz') as tar:
                tar.extractall(data_dir)
        elif archive_path.suffix == '.tar':
            with tarfile.open(archive_path, 'r:') as tar:
                tar.extractall(data_dir)
        else:
            print(f"‚ùå Unsupported archive format: {archive_path.suffix}")
            return None

        # Find the extracted directory
        extracted_path = data_dir / "MRBrainS13"
        if extracted_path.exists():
            print(f"‚úÖ Extracted to: {extracted_path}")
            return extracted_path

        # Check for common MRBrainS dataset folder names
        possible_names = ['MRBrainS13DataNii', 'TrainingData', 'training', 'MRBrainS']
        for name in possible_names:
            check_path = data_dir / name
            if check_path.exists():
                print(f"‚úÖ Extracted to: {check_path}")
                return check_path

        # Check what was actually extracted
        for item in data_dir.iterdir():
            if item.is_dir() and item.name not in ['__pycache__', 'models', 'results', 'visualizations', 'checkpoints', '.ipynb_checkpoints']:
                print(f"‚úÖ Extracted to: {item}")
                return item

        # If nothing found, return data_dir itself
        print(f"‚ö†Ô∏è No specific subdirectory found, using: {data_dir}")
        return data_dir


    except Exception as e:
        print(f"‚ùå Error extracting: {e}")
        return None


# Download and extract
dataset_file = download_dataset(directories['data'])
dataset_path = extract_dataset(dataset_file, directories['data'])

# ============================================================================
# STEP 4: Explore Dataset Structure
# ============================================================================

def explore_dataset(dataset_path):
    """Explore the dataset structure and load configuration"""

    if not dataset_path or not dataset_path.exists():
        print("‚ùå Dataset path not found!")
        return None, None

    print("\nüìã Exploring MRBrainS13 Dataset Structure...")

    # Find all NIfTI files
    nii_files = list(dataset_path.rglob("*.nii")) + list(dataset_path.rglob("*.nii.gz"))
    print(f"   Found {len(nii_files)} NIfTI files")

    # Display directory structure
    print(f"\nüìÅ Directory structure:")
    for item in dataset_path.rglob("*"):
        if item.is_dir():
            level = len(item.relative_to(dataset_path).parts)
            indent = "   " * level
            print(f"{indent}üìÅ {item.name}")

    # Try to find training data patterns
    dataset_info = {}

    # Look for subject folders
    subject_folders = []
    for item in dataset_path.iterdir():
        if item.is_dir() and item.name not in ['__pycache__', '.ipynb_checkpoints']:
            subject_folders.append(item)

    # Check for MRBrainS typical structure
    subdirs = ['imagesTr', 'labelsTr', 'imagesTs', 'TrainingData', 'training']
    for subdir in subdirs:
        subdir_path = dataset_path / subdir
        if subdir_path.exists():
            files = list(subdir_path.glob("*.nii.gz")) + list(subdir_path.glob("*.nii"))
            dataset_info[subdir] = {
                'path': subdir_path,
                'files': files,
                'count': len(files)
            }
            print(f"   üìÅ {subdir}: {len(files)} files")

    # If no standard structure, look for subject-based structure
    if not dataset_info and subject_folders:
        print(f"\n   Found {len(subject_folders)} subject folders:")
        images = []
        labels = []

        for subject in subject_folders:
            print(f"   üìÅ {subject.name}")
            subj_files = list(subject.glob("*.nii*"))

            for f in subj_files:
                if 'seg' in f.name.lower() or 'label' in f.name.lower():
                    labels.append(f)
                else:
                    images.append(f)

        dataset_info['imagesTr'] = {
            'path': dataset_path,
            'files': images,
            'count': len(images)
        }
        dataset_info['labelsTr'] = {
            'path': dataset_path,
            'files': labels,
            'count': len(labels)
        }
        dataset_info['imagesTs'] = {'path': None, 'files': [], 'count': 0}

        print(f"\n   üìä Found {len(images)} training images")
        print(f"   üìä Found {len(labels)} label files")

    # Create a basic config
    config = {
        'name': 'MRBrainS13',
        'description': 'Brain MRI Segmentation',
        'modality': 'MRI',
        'numTraining': dataset_info.get('imagesTr', {}).get('count', 0),
        'numTest': dataset_info.get('imagesTs', {}).get('count', 0),
        'labels': {
            '0': 'Background',
            '1': 'Grey Matter',
            '2': 'White Matter',
            '3': 'CSF'
        }
    }

    return dataset_info, config

# Explore dataset
if dataset_path:
    dataset_info, dataset_config = explore_dataset(dataset_path)

    # Save paths for next parts
    if dataset_info: # Check if dataset_info is not empty
        with open(directories['base'] / 'paths.json', 'w') as f:
            json.dump({
                'dataset_path': str(dataset_path),
                'directories': {k: str(v) for k, v in directories.items()},
                'train_images': len(dataset_info.get('imagesTr', {}).get('files', [])) if 'imagesTr' in dataset_info else 0, # Use .get() with default empty list
                'train_labels': len(dataset_info.get('labelsTr', {}).get('files', [])) if 'labelsTr' in dataset_info else 0, # Use .get() with default empty list
                'model_type': 'deeplabv3_resnet101'
            }, f, indent=2)

        print(f"\nüéâ Part 1 Complete!")
        print(f"‚úÖ Dataset ready with {dataset_info.get('imagesTr', {}).get('count', 0)} training images") # Use .get() with default 0
        print(f"üìç All paths saved to: {directories['base'] / 'paths.json'}")
        print(f"ü§ñ Model: DeepLabV3+ with ResNet-101 backbone")

    else:
        print("‚ùå Failed to setup dataset. Please check the download URL and extracted file structure.")

else:
    print("‚ùå Failed to setup dataset. Please check the download URL.")

print("\nüéØ Next: Run Part 2 - Data Preprocessing and DeepLab Model Setup")

# Part 2: Data Preprocessing and DeepLab Model Setup
# Run after Part 1

import numpy as np
import matplotlib.pyplot as plt
import nibabel as nib
from pathlib import Path
import json
from skimage import measure, filters
from scipy import ndimage
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import torch
import torch.nn as nn
import torchvision.models.segmentation as models
from PIL import Image
import cv2

print("üöÄ Part 2: Data Preprocessing and DeepLab Model Setup")
print("="*50)

# Load paths from Part 1
with open('/content/brain_segmentation/paths.json', 'r') as f:
    paths_info = json.load(f)

dataset_path = Path(paths_info['dataset_path'])
directories = {k: Path(v) for k, v in paths_info['directories'].items()}

# ============================================================================
# STEP 1: Data Loading and Preprocessing Functions
# ============================================================================

def load_nifti_data(image_path, label_path=None):
    """Load NIfTI image and optional label"""

    # Load image
    image_nii = nib.load(image_path)
    image_data = image_nii.get_fdata()

    result = {
        'image': image_data,
        'image_nii': image_nii,
        'spacing': image_nii.header.get_zooms(),
        'affine': image_nii.affine
    }

    # Load label if provided
    if label_path and Path(label_path).exists():
        label_nii = nib.load(label_path)
        label_data = label_nii.get_fdata()
        result['label'] = label_data
        result['label_nii'] = label_nii

    return result

def preprocess_image(image_data, percentile_lower=1, percentile_upper=99):
    """Apply MRI intensity normalization"""

    # Normalize MRI intensity using percentiles
    lower = np.percentile(image_data, percentile_lower)
    upper = np.percentile(image_data, percentile_upper)

    # Clip and normalize
    clipped = np.clip(image_data, lower, upper)
    normalized = (clipped - lower) / (upper - lower)

    return normalized

def preprocess_for_deeplab(image_slice, target_size=(512, 512)):
    """Preprocess a single 2D slice for DeepLab input"""

    # Resize to target size
    resized = cv2.resize(image_slice, target_size, interpolation=cv2.INTER_LINEAR)

    # Convert to 3-channel (DeepLab expects RGB input)
    rgb_image = np.stack([resized, resized, resized], axis=-1)

    # Normalize to ImageNet statistics (DeepLab pre-training)
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    normalized = (rgb_image - mean) / std

    # Convert to tensor (C, H, W)
    tensor = torch.from_numpy(normalized).float().permute(2, 0, 1)

    return tensor, resized

def preprocess_label(label_slice, target_size=(512, 512)):
    """Preprocess label for DeepLab training"""

    # Resize label using nearest neighbor to preserve class values
    resized = cv2.resize(label_slice.astype(np.float32), target_size,
                        interpolation=cv2.INTER_NEAREST)

    return torch.from_numpy(resized).long()

def identify_brain_parts(label_data):
    """Identify different parts of brain segmentation"""

    unique_labels = np.unique(label_data)
    print(f"üìä Found labels: {unique_labels}")

    # For MRBrainS13: 0=background, 1=grey matter, 2=white matter, 3=CSF
    parts = {
        0: {'name': 'Background', 'color': 'rgba(0,0,0,0)'},
        1: {'name': 'Grey Matter', 'color': 'rgba(128,128,128,0.7)'},     # Gray
        2: {'name': 'White Matter', 'color': 'rgba(255,255,255,0.7)'},    # White
        3: {'name': 'CSF', 'color': 'rgba(0,191,255,0.7)'}                # Deep Sky Blue
    }

    return parts, unique_labels

# ============================================================================
# STEP 2: Load DeepLab Model
# ============================================================================

def load_deeplab_model(num_classes=4, pretrained=True):
    """Load pre-trained DeepLabV3+ model"""

    print("ü§ñ Loading DeepLabV3 with ResNet-101 backbone...")

    # Load pre-trained DeepLabV3
    model = models.deeplabv3_resnet101(pretrained=pretrained)

    # Modify classifier for our number of classes
    # DeepLabV3 has a classifier with an output layer we need to modify
    in_channels = model.classifier[4].in_channels
    model.classifier[4] = nn.Conv2d(in_channels, num_classes, kernel_size=1)

    # Also modify the auxiliary classifier if it exists
    if hasattr(model, 'aux_classifier'):
        in_channels_aux = model.aux_classifier[4].in_channels
        model.aux_classifier[4] = nn.Conv2d(in_channels_aux, num_classes, kernel_size=1)

    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    print(f"‚úÖ Model loaded on: {device}")
    print(f"   Number of classes: {num_classes}")
    print(f"   Input size: (3, H, W) - RGB images")

    return model, device

# Load DeepLab model
deeplab_model, device = load_deeplab_model(num_classes=4)

# Save model info
model_info = {
    'model_type': 'deeplabv3_resnet101',
    'num_classes': 4,
    'input_size': [512, 512],
    'device': str(device),
    'pretrained': True
}

with open(directories['models'] / 'model_config.json', 'w') as f:
    json.dump(model_info, f, indent=2)

# ============================================================================
# STEP 3: Load and Visualize Sample Data
# ============================================================================

def load_sample_case(dataset_path, case_index=0):
    """Load a sample case for demonstration"""

    print(f"üîç Exploring dataset structure at: {dataset_path}")

    # Check if dataset_path itself is the training directory (contains subject folders)
    potential_subjects = [item for item in dataset_path.iterdir() if item.is_dir() and not item.name.startswith('.')]

    # If we find numbered folders or folders with subject data, use dataset_path directly
    if potential_subjects:
        # Check if these look like subject folders
        first_folder = potential_subjects[0]
        nii_files = list(first_folder.glob("*.nii*"))
        if nii_files:
            print(f"‚úÖ Dataset path contains subject folders directly")
            training_dir = dataset_path
        else:
            # Look for TrainingData subfolder
            training_dir = dataset_path / "TrainingData"
    else:
        training_dir = dataset_path / "TrainingData"

    if not training_dir.exists():
        # Try alternative paths
        possible_dirs = ['TrainingData', 'training', 'train', 'Training']
        for dirname in possible_dirs:
            check_dir = dataset_path / dirname
            if check_dir.exists():
                training_dir = check_dir
                break

    print(f"‚úÖ Using training directory: {training_dir}")

    # Find subject folders or NIfTI files
    subject_folders = sorted([f for f in training_dir.iterdir() if f.is_dir() and not f.name.startswith('.')])

    if not subject_folders:
        print("‚ö†Ô∏è No subject folders found")
        return None

    print(f"‚úÖ Found {len(subject_folders)} subject folders: {[f.name for f in subject_folders]}")

    # Select case
    if case_index >= len(subject_folders):
        case_index = 0

    subject_folder = subject_folders[case_index]
    print(f"\nüìÅ Loading case {case_index}: {subject_folder.name}")

    # Find image and label files - look for segmentation files
    all_files = list(subject_folder.iterdir())
    print(f"   üìÇ Files in subject folder: {[f.name for f in all_files]}")

    # MRBrainS13 typically has segmentation files, look for them
    image_files = [f for f in all_files if f.suffix in ['.nii', '.gz'] and 'seg' not in f.name.lower() and 'label' not in f.name.lower()]
    label_files = [f for f in all_files if f.suffix in ['.nii', '.gz'] and ('seg' in f.name.lower() or 'label' in f.name.lower() or 'labelsfor' in f.name.lower())]

    # If no label files found with those patterns, check if there's a LabelsForTesting or similar
    if not label_files:
        # Look in parent directory for labels
        parent_labels = list(training_dir.glob(f"*{subject_folder.name}*.nii*"))
        parent_labels += list(training_dir.parent.glob(f"*{subject_folder.name}*.nii*"))
        label_files = [f for f in parent_labels if 'label' in f.name.lower() or 'seg' in f.name.lower()]

    if not image_files:
        print("‚ùå No image files found!")
        print(f"   Searched in: {subject_folder}")
        print(f"   Files present: {[f.name for f in subject_folder.iterdir()]}")
        return None

    if not label_files:
        print("‚ùå No label files found!")
        print(f"   Searched in: {subject_folder}")
        print(f"   Files present: {[f.name for f in subject_folder.iterdir()]}")
        return None

    image_path = image_files[0]
    label_path = label_files[0]

    print(f"   üìÑ Image: {image_path.name}")
    print(f"   üìÑ Label: {label_path.name}")

    # Load data
    data = load_nifti_data(image_path, label_path)

    # Preprocess
    data['image_processed'] = preprocess_image(data['image'])

    # Identify brain parts
    data['parts'], data['unique_labels'] = identify_brain_parts(data['label'])

    # Print info
    print(f"   üìä Image shape: {data['image'].shape}")
    print(f"   üìä Spacing: {data['spacing']}")
    print(f"   üìä Value range: {data['image'].min():.1f} to {data['image'].max():.1f}")
    print(f"   üéØ Labels found: {data['unique_labels']}")

    return data

# Load sample case
sample_data = load_sample_case(dataset_path, case_index=0)

# ============================================================================
# STEP 4: Test DeepLab on Sample Slice
# ============================================================================

def test_deeplab_inference(model, device, image_slice):
    """Test DeepLab inference on a sample slice"""

    print("üî¨ Testing DeepLab inference on sample slice...")

    # Preprocess
    input_tensor, _ = preprocess_for_deeplab(image_slice)
    input_batch = input_tensor.unsqueeze(0).to(device)

    # Inference
    model.eval()
    with torch.no_grad():
        output = model(input_batch)['out']
        prediction = output.argmax(1).squeeze().cpu().numpy()

    print(f"‚úÖ Inference successful!")
    print(f"   Input shape: {input_batch.shape}")
    print(f"   Output shape: {output.shape}")
    print(f"   Prediction shape: {prediction.shape}")
    print(f"   Predicted classes: {np.unique(prediction)}")

    return prediction

# Test on middle slice
if sample_data:
    mid_slice = sample_data['image_processed'][:, :, sample_data['image_processed'].shape[2]//2]
    test_prediction = test_deeplab_inference(deeplab_model, device, mid_slice)

# ============================================================================
# STEP 5: 2D Slice Visualization with DeepLab Prediction
# ============================================================================

def visualize_slices_with_prediction(data, model, device, slice_indices=None):
    """Visualize 2D slices with brain segmentation and DeepLab prediction"""

    if data is None:
        return

    image = data['image_processed']
    label = data['label']
    parts = data['parts']

    # Select slices to show
    if slice_indices is None:
        depth = image.shape[2]
        slice_indices = [depth//4, depth//2, 3*depth//4]

    fig, axes = plt.subplots(3, 4, figsize=(20, 15))

    for i, slice_idx in enumerate(slice_indices):
        # Get slices
        img_slice = image[:, :, slice_idx]
        lbl_slice = label[:, :, slice_idx]

        # Get DeepLab prediction (before fine-tuning)
        with torch.no_grad():
            input_tensor, _ = preprocess_for_deeplab(img_slice)
            input_batch = input_tensor.unsqueeze(0).to(device)
            model.eval()
            output = model(input_batch)['out']
            pred_slice = output.argmax(1).squeeze().cpu().numpy()

        # Original image
        axes[i, 0].imshow(img_slice.T, cmap='gray', origin='lower')
        axes[i, 0].set_title(f'MRI Image - Slice {slice_idx}')
        axes[i, 0].axis('off')

        # Ground truth overlay
        axes[i, 1].imshow(img_slice.T, cmap='gray', origin='lower')
        for label_val, part_info in parts.items():
            if label_val == 0:
                continue
            mask = (lbl_slice == label_val)
            if np.any(mask):
                rgb_vals = part_info['color'].replace('rgba(', '').replace(')', '').split(',')[:3]
                rgb_color = [float(x)/255 for x in rgb_vals]
                axes[i, 1].contour(mask.T, colors=[rgb_color], linewidths=2, origin='lower')
        axes[i, 1].set_title(f'Ground Truth - Slice {slice_idx}')
        axes[i, 1].axis('off')

        # Ground truth mask
        colored_label = np.zeros((*lbl_slice.shape, 3))
        for label_val, part_info in parts.items():
            if label_val == 0:
                continue
            mask = (lbl_slice == label_val)
            if np.any(mask):
                if label_val == 1:  # Grey Matter
                    colored_label[mask] = [0.5, 0.5, 0.5]
                elif label_val == 2:  # White Matter
                    colored_label[mask] = [1.0, 1.0, 1.0]
                elif label_val == 3:  # CSF
                    colored_label[mask] = [0.0, 0.75, 1.0]
        axes[i, 2].imshow(colored_label.transpose(1, 0, 2), origin='lower')
        axes[i, 2].set_title(f'GT Mask - Slice {slice_idx}')
        axes[i, 2].axis('off')

        # DeepLab prediction (pre-training)
        colored_pred = np.zeros((*pred_slice.shape, 3))
        colored_pred[pred_slice == 1] = [0.5, 0.5, 0.5]  # Grey Matter
        colored_pred[pred_slice == 2] = [1.0, 1.0, 1.0]  # White Matter
        colored_pred[pred_slice == 3] = [0.0, 0.75, 1.0]  # CSF
        axes[i, 3].imshow(colored_pred.transpose(1, 0, 2), origin='lower')
        axes[i, 3].set_title(f'DeepLab (Pre-trained) - Slice {slice_idx}')
        axes[i, 3].axis('off')

    plt.suptitle('Before Fine-tuning: DeepLab needs training on medical data', fontsize=14, y=1.00)
    plt.tight_layout()
    plt.savefig(directories['visualizations'] / 'sample_slices_with_prediction.png', dpi=150, bbox_inches='tight')
    plt.show()

    # Print statistics
    print(f"\nüìà Ground Truth Segmentation Statistics:")
    for label_val, part_info in parts.items():
        if label_val == 0:
            continue
        mask = (label == label_val)
        volume = np.sum(mask) * np.prod(data['spacing'])
        percentage = (np.sum(mask) / label.size) * 100
        print(f"   {part_info['name']}: {volume/1000:.1f} cm¬≥ ({percentage:.2f}%)")

# Visualize with predictions
if sample_data:
    visualize_slices_with_prediction(sample_data, deeplab_model, device)

# ============================================================================
# STEP 6: 3D Preview
# ============================================================================

def create_3d_preview(data, downsample_factor=4):
    """Create a simple 3D preview of the brain segmentation"""

    if data is None:
        return

    image = data['image_processed']
    label = data['label']

    # Downsample for faster processing
    label_small = label[::downsample_factor, ::downsample_factor, ::downsample_factor]

    print(f"üé≤ Creating 3D preview (downsampled to {label_small.shape})")

    fig = make_subplots(
        rows=1, cols=3,
        subplot_titles=('Grey Matter', 'White Matter', 'CSF'),
        specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}, {'type': 'scatter3d'}]]
    )

    # Grey Matter coordinates
    grey_coords = np.where(label_small == 1)
    if len(grey_coords[0]) > 0:
        sample_size = min(5000, len(grey_coords[0]))
        indices = np.random.choice(len(grey_coords[0]), sample_size, replace=False)
        x, y, z = [grey_coords[i][indices] for i in range(3)]

        fig.add_trace(
            go.Scatter3d(
                x=x, y=y, z=z,
                mode='markers',
                marker=dict(size=2, color='gray', opacity=0.6),
                name='Grey Matter',
                showlegend=True
            ),
            row=1, col=1
        )

    # White Matter coordinates
    white_coords = np.where(label_small == 2)
    if len(white_coords[0]) > 0:
        sample_size = min(5000, len(white_coords[0]))
        indices = np.random.choice(len(white_coords[0]), sample_size, replace=False)
        x_w, y_w, z_w = [white_coords[i][indices] for i in range(3)]
        fig.add_trace(
            go.Scatter3d(
                x=x_w, y=y_w, z=z_w,
                mode='markers',
                marker=dict(size=2, color='white', opacity=0.6),
                name='White Matter',
                showlegend=True
            ),
            row=1, col=2
        )

    # CSF coordinates
    csf_coords = np.where(label_small == 3)
    if len(csf_coords[0]) > 0:
        x_c, y_c, z_c = csf_coords
        fig.add_trace(
            go.Scatter3d(
                x=x_c, y=y_c, z=z_c,
                mode='markers',
                marker=dict(size=2, color='deepskyblue', opacity=0.8),
                name='CSF',
                showlegend=True
            ),
            row=1, col=3
        )

    fig.update_layout(
        title="3D Brain Segmentation Preview",
        scene=dict(aspectmode='data'),
        scene2=dict(aspectmode='data'),
        scene3=dict(aspectmode='data'),
        height=600
    )

    fig.show()

# Create 3D preview
if sample_data:
    create_3d_preview(sample_data)

# ============================================================================
# STEP 7: Save Preprocessed Data
# ============================================================================

def save_sample_data(data, save_path):
    """Save preprocessed sample data"""

    if data is None:
        return

    np.savez_compressed(
        save_path / 'sample_data.npz',
        image=data['image'],
        image_processed=data['image_processed'],
        label=data['label'],
        spacing=data['spacing'],
        unique_labels=data['unique_labels']
    )

    metadata = {
        'parts': data['parts'],
        'shape': list(data['image'].shape),
        'spacing': [float(s) for s in data['spacing']],
        'unique_labels': data['unique_labels'].tolist(),
        'model': 'deeplabv3_resnet101'
    }

    with open(save_path / 'sample_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"üíæ Sample data saved to: {save_path}")

# Save data
if sample_data:
    save_sample_data(sample_data, directories['results'])

print(f"\nüéâ Part 2 Complete!")
print(f"‚úÖ Sample data loaded and visualized")
print(f"‚úÖ DeepLab model loaded and tested")
print(f"‚úÖ Pre-trained model shows random predictions (needs fine-tuning)")
print(f"üìÅ Visualizations saved to: {directories['visualizations']}")
print(f"\nüéØ Next: Run Part 3 - Fine-tuning DeepLab on Brain Data")

# Part 3: Fine-tuning DeepLab on Brain Dataset
# Run after Part 2

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
from pathlib import Path
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

print("üöÄ Part 3: Fine-tuning DeepLab on Brain Data")
print("="*40)

# Check GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üñ•Ô∏è Using device: {device}")

# Load paths and data
with open('/content/brain_segmentation/paths.json', 'r') as f:
    paths_info = json.load(f)

directories = {k: Path(v) for k, v in paths_info['directories'].items()}
dataset_path = Path(paths_info['dataset_path'])

# Load sample data
sample_data_path = directories['results'] / 'sample_data.npz'
if sample_data_path.exists():
    print("üìÇ Loading preprocessed sample data...")
    loaded_data = np.load(sample_data_path)
    sample_image = loaded_data['image_processed']
    sample_label = loaded_data['label']
    spacing = loaded_data['spacing']
    print(f"‚úÖ Loaded data shape: {sample_image.shape}")
else:
    print("‚ùå No preprocessed data found. Run Part 2 first!")
    sample_image = sample_label = spacing = None

# ============================================================================
# STEP 1: Create Custom Dataset for DeepLab Training
# ============================================================================

class BrainDataset(Dataset):
    """Custom Dataset for brain MRI scans"""

    def __init__(self, image_3d, label_3d, target_size=(256, 256),
                 min_brain_pixels=500, transform=None):
        """
        Args:
            image_3d: 3D preprocessed MRI volume (H, W, D)
            label_3d: 3D label volume (H, W, D)
            target_size: Target size for resizing
            min_brain_pixels: Minimum brain pixels to include slice
            transform: Optional transforms
        """
        self.target_size = target_size
        self.transform = transform

        # Extract valid slices (with sufficient brain content)
        self.slices = []
        for z in range(image_3d.shape[2]):
            brain_pixels = np.sum(label_3d[:, :, z] > 0)
            if brain_pixels >= min_brain_pixels:
                self.slices.append({
                    'image': image_3d[:, :, z],
                    'label': label_3d[:, :, z],
                    'slice_idx': z
                })

        print(f"   Created dataset with {len(self.slices)} valid slices")

    def __len__(self):
        return len(self.slices)

    def __getitem__(self, idx):
        slice_data = self.slices[idx]
        image = slice_data['image']
        label = slice_data['label']

        # Resize
        image_resized = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LINEAR)
        label_resized = cv2.resize(label.astype(np.float32), self.target_size,
                                   interpolation=cv2.INTER_NEAREST)

        # Convert to 3-channel RGB
        image_rgb = np.stack([image_resized, image_resized, image_resized], axis=-1)

        # Normalize with ImageNet statistics
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image_normalized = (image_rgb - mean) / std

        # Convert to tensors (C, H, W)
        image_tensor = torch.from_numpy(image_normalized).float().permute(2, 0, 1)
        label_tensor = torch.from_numpy(label_resized).long()

        return image_tensor, label_tensor, slice_data['slice_idx']

# ============================================================================
# STEP 2: Load DeepLab Model
# ============================================================================

def load_deeplab_for_training(num_classes=4, pretrained=True):
    """Load DeepLabV3 model for fine-tuning"""

    print("ü§ñ Loading DeepLabV3 model for training...")

    import torchvision.models.segmentation as models

    # Load pre-trained model
    model = models.deeplabv3_resnet101(pretrained=pretrained)

    # Modify classifier for our classes
    in_channels = model.classifier[4].in_channels
    model.classifier[4] = nn.Conv2d(in_channels, num_classes, kernel_size=1)

    # Modify auxiliary classifier
    if hasattr(model, 'aux_classifier'):
        in_channels_aux = model.aux_classifier[4].in_channels
        model.aux_classifier[4] = nn.Conv2d(in_channels_aux, num_classes, kernel_size=1)

    model = model.to(device)
    print(f"‚úÖ Model loaded on {device}")

    return model

# Load model
deeplab_model = load_deeplab_for_training(num_classes=4, pretrained=True)

# ============================================================================
# STEP 3: Training Setup
# ============================================================================

def calculate_class_weights(label_3d):
    """Calculate class weights for handling class imbalance"""

    unique, counts = np.unique(label_3d, return_counts=True)
    total = label_3d.size

    weights = []
    for i in range(4):  # 0: background, 1: grey matter, 2: white matter, 3: CSF
        if i in unique:
            weight = total / (len(unique) * counts[unique == i][0])
            weights.append(weight)
        else:
            weights.append(1.0)

    # Normalize weights
    weights = np.array(weights)
    weights = weights / weights.sum() * len(weights)

    print(f"üìä Class weights: {weights}")
    return torch.FloatTensor(weights).to(device)

class DiceLoss(nn.Module):
    """Dice loss for segmentation"""

    def __init__(self, smooth=1.0):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, pred, target, num_classes=4):
        pred = torch.softmax(pred, dim=1)

        dice_sum = 0
        for c in range(num_classes):
            pred_c = pred[:, c, :, :]
            target_c = (target == c).float()

            intersection = (pred_c * target_c).sum()
            union = pred_c.sum() + target_c.sum()

            dice = (2. * intersection + self.smooth) / (union + self.smooth)
            dice_sum += dice

        return 1 - dice_sum / num_classes

class CombinedLoss(nn.Module):
    """Combined Cross Entropy and Dice Loss"""

    def __init__(self, weight=None, ce_weight=0.5, dice_weight=0.5):
        super(CombinedLoss, self).__init__()
        self.ce_loss = nn.CrossEntropyLoss(weight=weight)
        self.dice_loss = DiceLoss()
        self.ce_weight = ce_weight
        self.dice_weight = dice_weight

    def forward(self, pred, target):
        ce = self.ce_loss(pred, target)
        dice = self.dice_loss(pred, target)
        return self.ce_weight * ce + self.dice_weight * dice

# Setup training components
if sample_image is not None:
# Create dataset
    train_dataset = BrainDataset(sample_image, sample_label, target_size=(256, 256))
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,
                             num_workers=0, drop_last=True)

    # Calculate class weights
    class_weights = calculate_class_weights(sample_label)

    # Setup loss and optimizer
    criterion = CombinedLoss(weight=class_weights, ce_weight=0.5, dice_weight=0.5)
    optimizer = optim.Adam(deeplab_model.parameters(), lr=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

# ============================================================================
# STEP 4: Training Function
# ============================================================================

def train_epoch(model, dataloader, criterion, optimizer, device):
    """Train for one epoch"""

    model.train()
    total_loss = 0
    num_batches = 0

    pbar = tqdm(dataloader, desc="Training")
    for images, labels, _ in pbar:
        images = images.to(device)
        labels = labels.to(device)

        # Forward pass
        optimizer.zero_grad()
        outputs = model(images)['out']

        # Calculate loss
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

        pbar.set_postfix({'loss': f'{loss.item():.4f}'})

    return total_loss / num_batches

def evaluate(model, dataloader, criterion, device):
    """Evaluate model"""

    model.eval()
    total_loss = 0
    num_batches = 0

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels, _ in tqdm(dataloader, desc="Evaluating"):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)['out']
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            num_batches += 1

            # Get predictions
            preds = outputs.argmax(1)
            all_preds.append(preds.cpu().numpy())
            all_labels.append(labels.cpu().numpy())

    return total_loss / num_batches, np.concatenate(all_preds), np.concatenate(all_labels)

# ============================================================================
# STEP 5: Train the Model
# ============================================================================

def train_deeplab(model, train_loader, criterion, optimizer, scheduler,
                  num_epochs=10, checkpoint_dir=None):
    """Full training loop"""

    print(f"üèãÔ∏è Starting training for {num_epochs} epochs...")

    history = {
        'train_loss': [],
        'val_loss': []
    }

    best_loss = float('inf')

    for epoch in range(num_epochs):
        print(f"\nüìÖ Epoch {epoch+1}/{num_epochs}")

        # Train
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        history['train_loss'].append(train_loss)

        # Note: In real scenario, you'd have separate validation set
        # For now, we'll use training loss
        val_loss = train_loss
        history['val_loss'].append(val_loss)

        # Learning rate scheduling
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']

        print(f"   Train Loss: {train_loss:.4f}")
        print(f"   Learning Rate: {current_lr:.6f}")

        # Save best model
        if val_loss < best_loss and checkpoint_dir:
            best_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': val_loss,
            }, checkpoint_dir / 'best_model.pth')
            print(f"   üíæ Best model saved (loss: {best_loss:.4f})")

    return history

# Train the model if data is available
training_history = None
if sample_image is not None:
    training_history = train_deeplab(
        deeplab_model,
        train_loader,
        criterion,
        optimizer,
        scheduler,
        num_epochs=10,
        checkpoint_dir=directories['checkpoints']
    )

# ============================================================================
# STEP 6: Visualize Training Progress
# ============================================================================

def plot_training_history(history, save_dir):
    """Plot training curves"""

    if history is None:
        return

    plt.figure(figsize=(10, 5))
    plt.plot(history['train_loss'], label='Training Loss', marker='o')
    plt.plot(history['val_loss'], label='Validation Loss', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('DeepLab Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_dir / 'training_history.png', dpi=150, bbox_inches='tight')
    plt.show()

    print(f"üìà Training completed!")
    print(f"   Final train loss: {history['train_loss'][-1]:.4f}")
    print(f"   Best val loss: {min(history['val_loss']):.4f}")

if training_history:
    plot_training_history(training_history, directories['results'])

# ============================================================================
# STEP 7: Inference on Sample Slices
# ============================================================================

def inference_on_slices(model, image_3d, label_3d, num_slices=5):
    """Run inference on sample slices"""

    print(f"üîç Running inference on {num_slices} slices...")

    model.eval()
    results = {}

    # Select slices with brain content
    brain_slices = []
    for z in range(image_3d.shape[2]):
        brain_area = np.sum(label_3d[:, :, z] > 0)
        if brain_area > 500:
            brain_slices.append((z, brain_area))

    brain_slices.sort(key=lambda x: x[1], reverse=True)
    selected_slices = [s[0] for s in brain_slices[:num_slices]]

    for z in tqdm(selected_slices, desc="Inference"):
        # Prepare slice
        image_slice = image_3d[:, :, z]
        label_slice = label_3d[:, :, z]

        # Resize and normalize
        image_resized = cv2.resize(image_slice, (256, 256))
        image_rgb = np.stack([image_resized, image_resized, image_resized], axis=-1)

        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image_normalized = (image_rgb - mean) / std

        # Convert to tensor
        image_tensor = torch.from_numpy(image_normalized).float().permute(2, 0, 1).unsqueeze(0)

        # Inference
        with torch.no_grad():
            output = model(image_tensor.to(device))['out']
            prediction = output.argmax(1).squeeze().cpu().numpy()

        # Resize back to original size
        prediction_resized = cv2.resize(prediction.astype(np.float32),
                                       (image_slice.shape[1], image_slice.shape[0]),
                                       interpolation=cv2.INTER_NEAREST)

        results[z] = {
            'original': image_slice,
            'ground_truth': label_slice,
            'prediction': prediction_resized
        }

    print(f"‚úÖ Inference completed on {len(results)} slices")
    return results

# Run inference
inference_results = None
if sample_image is not None:
    inference_results = inference_on_slices(deeplab_model, sample_image, sample_label, num_slices=5)

# ============================================================================
# STEP 8: Visualize Results
# ============================================================================

def visualize_results(results, save_dir):
    """Visualize segmentation results"""

    if not results:
        return

    slice_keys = list(results.keys())[:3]

    fig, axes = plt.subplots(len(slice_keys), 3, figsize=(15, 5*len(slice_keys)))
    if len(slice_keys) == 1:
        axes = axes.reshape(1, -1)

    for i, slice_z in enumerate(slice_keys):
        result = results[slice_z]

        # Original
        axes[i, 0].imshow(result['original'].T, cmap='gray', origin='lower')
        axes[i, 0].set_title(f'MRI Image - Slice {slice_z}')
        axes[i, 0].axis('off')

        # Ground truth
        gt_colored = np.zeros((*result['ground_truth'].shape, 3))
        gt_colored[result['ground_truth'] == 1] = [0.5, 0.5, 0.5]  # Grey Matter
        gt_colored[result['ground_truth'] == 2] = [1.0, 1.0, 1.0]  # White Matter
        gt_colored[result['ground_truth'] == 3] = [0.0, 0.75, 1.0]  # CSF
        axes[i, 1].imshow(gt_colored.transpose(1, 0, 2), origin='lower')
        axes[i, 1].set_title(f'Ground Truth - Slice {slice_z}')
        axes[i, 1].axis('off')

        # Prediction
        pred_colored = np.zeros((*result['prediction'].shape, 3))
        pred_colored[result['prediction'] == 1] = [0.5, 0.5, 0.5]
        pred_colored[result['prediction'] == 2] = [1.0, 1.0, 1.0]
        pred_colored[result['prediction'] == 3] = [0.0, 0.75, 1.0]
        axes[i, 2].imshow(pred_colored.transpose(1, 0, 2), origin='lower')
        axes[i, 2].set_title(f'DeepLab Prediction - Slice {slice_z}')
        axes[i, 2].axis('off')

    plt.suptitle('DeepLab Fine-tuned Results', fontsize=14, y=0.995)
    plt.tight_layout()
    plt.savefig(save_dir / 'deeplab_results.png', dpi=150, bbox_inches='tight')
    plt.show()

if inference_results:
    visualize_results(inference_results, directories['results'])

# ============================================================================
# STEP 9: Save Model and Results
# ============================================================================

# Save final model
if sample_image is not None:
    torch.save({
        'model_state_dict': deeplab_model.state_dict(),
        'model_config': {
            'type': 'deeplabv3_resnet101',
            'num_classes': 4,
            'input_size': [256, 256]
        }
    }, directories['models'] / 'deeplab_finetuned.pth')

    print(f"üíæ Model saved to: {directories['models'] / 'deeplab_finetuned.pth'}")

print(f"\nüéâ Part 3 Complete!")
print(f"‚úÖ DeepLab model fine-tuned on brain data")
print(f"‚úÖ Training history and results saved")
if inference_results:
    print(f"‚úÖ Successfully processed {len(inference_results)} slices")
print(f"\nüéØ Next: Run Part 4 - Dice Score Calculation and 3D Visualization")

# Part 4: 3-Part Brain Segmentation with Different Colors

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from skimage import measure, morphology, segmentation
from scipy import ndimage
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from scipy.spatial.distance import cdist

print("üöÄ Part 4: 3-Part Brain Segmentation Analysis")
print("="*50)

# Load data from previous steps
directories = {k: Path(v) for k, v in json.load(open('/content/brain_segmentation/paths.json'))['directories'].items()}

# Load sample data
sample_data_path = directories['results'] / 'sample_data.npz'
loaded_data = np.load(sample_data_path)
sample_image = loaded_data['image_processed']
sample_label = loaded_data['label']
spacing = loaded_data['spacing']

print(f"üìÇ Loaded data - Shape: {sample_label.shape}, Spacing: {spacing}")

# ============================================================================
# STEP 1: Extract 3 Brain Parts (Grey Matter, White Matter, CSF)
# ============================================================================

def extract_3_brain_parts(brain_label):
    """Extract the 3 brain tissue types from segmentation"""

    print(f"üß† Extracting 3 brain tissue types...")

    # Create 3-part segmentation
    brain_3parts = brain_label.copy().astype(np.uint8)

    # Part names matching the segmentation
    part_names = {
        1: 'Grey Matter',
        2: 'White Matter',
        3: 'CSF'
    }

    # Calculate volumes for each part
    part_volumes = {}
    for part_id in [1, 2, 3]:
        part_mask = (brain_3parts == part_id)
        volume_mm3 = np.sum(part_mask) * np.prod(spacing)
        volume_cm3 = volume_mm3 / 1000
        part_volumes[part_id] = volume_cm3

        print(f"   Part {part_id} ({part_names[part_id]}): {volume_cm3:.1f} cm¬≥")

    return brain_3parts, part_names, part_volumes

# Create 3-part brain segmentation
brain_3parts, part_names, part_volumes = extract_3_brain_parts(sample_label)

# ============================================================================
# STEP 2: Visualize 3-Part Segmentation
# ============================================================================

def visualize_3part_segmentation(original_image, brain_3parts, part_names, slice_indices=None):
    """Visualize the 3-part brain segmentation"""

    if brain_3parts is None:
        return

    # Define colors for each part
    colors = {
        1: {'color': [0.5, 0.5, 0.5], 'name': 'Grey Matter', 'rgb': 'gray'},      # Gray
        2: {'color': [1.0, 1.0, 1.0], 'name': 'White Matter', 'rgb': 'white'},    # White
        3: {'color': [0.0, 0.75, 1.0], 'name': 'CSF', 'rgb': 'cyan'}              # Cyan
    }

    # Select slices
    if slice_indices is None:
        depth = original_image.shape[2]
        slice_indices = [depth//4, depth//2, 3*depth//4]

    fig, axes = plt.subplots(len(slice_indices), 4, figsize=(20, 5*len(slice_indices)))
    if len(slice_indices) == 1:
        axes = axes.reshape(1, -1)

    for i, slice_idx in enumerate(slice_indices):
        img_slice = original_image[:, :, slice_idx]
        seg_slice = brain_3parts[:, :, slice_idx]

        # Original image
        axes[i, 0].imshow(img_slice.T, cmap='gray', origin='lower')
        axes[i, 0].set_title(f'Original MRI - Slice {slice_idx}')
        axes[i, 0].axis('off')

        # Segmentation overlay
        axes[i, 1].imshow(img_slice.T, cmap='gray', origin='lower')
        for part_id, color_info in colors.items():
            mask = (seg_slice == part_id)
            if np.any(mask):
                axes[i, 1].contour(mask.T, colors=[color_info['color']],
                                 linewidths=3, origin='lower', alpha=0.8)

        axes[i, 1].set_title(f'3-Part Brain Segmentation - Slice {slice_idx}')
        axes[i, 1].axis('off')

        # Pure segmentation (colored)
        colored_seg = np.zeros((*seg_slice.shape, 3))
        for part_id, color_info in colors.items():
            mask = (seg_slice == part_id)
            colored_seg[mask] = color_info['color']

        axes[i, 2].imshow(colored_seg.transpose(1, 0, 2), origin='lower')
        axes[i, 2].set_title(f'Color-Coded Parts - Slice {slice_idx}')
        axes[i, 2].axis('off')

        # Individual parts breakdown
        axes[i, 3].imshow(img_slice.T, cmap='gray', origin='lower', alpha=0.5)

        # Show each part with different transparency
        for part_id, color_info in colors.items():
            mask = (seg_slice == part_id)
            if np.any(mask):
                colored_part = np.zeros((*mask.shape, 4))
                colored_part[mask] = [*color_info['color'], 0.7]
                axes[i, 3].imshow(colored_part.transpose(1, 0, 2), origin='lower')

        axes[i, 3].set_title(f'Layered Visualization - Slice {slice_idx}')
        axes[i, 3].axis('off')

    # Add legend
    legend_elements = []
    for part_id in [1, 2, 3]:
        color_info = colors[part_id]
        legend_elements.append(plt.Line2D([0], [0], color=color_info['color'],
                                        lw=4, label=f'Part {part_id}: {part_names[part_id]}'))

    fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.02),
              ncol=3, fontsize=12)

    plt.tight_layout()
    plt.savefig(directories['visualizations'] / '3_part_brain_segmentation.png',
                dpi=150, bbox_inches='tight')
    plt.show()

    # Print statistics
    print(f"\nüìä 3-Part Brain Statistics:")
    total_brain_volume = sum(part_volumes[i] for i in [1, 2, 3])
    for part_id in [1, 2, 3]:
        volume = part_volumes[part_id]
        percentage = (volume / total_brain_volume) * 100
        print(f"   {part_names[part_id]}: {volume:.1f} cm¬≥ ({percentage:.1f}%)")

# Visualize 3-part segmentation
if brain_3parts is not None:
    visualize_3part_segmentation(sample_image, brain_3parts, part_names)

# ============================================================================
# STEP 3: Dice Score Calculation for 3 Parts
# ============================================================================

def calculate_dice_score(pred_mask, true_mask, smooth=1e-6):
    """Calculate Dice Similarity Coefficient"""
    pred_flat = pred_mask.flatten()
    true_flat = true_mask.flatten()
    intersection = np.sum(pred_flat * true_flat)
    dice = (2.0 * intersection + smooth) / (np.sum(pred_flat) + np.sum(true_flat) + smooth)
    return dice

def create_simulated_3part_predictions(brain_3parts, noise_levels=[0.05, 0.15, 0.25]):
    """Create simulated predictions for the 3-part brain segmentation"""

    predictions = {}

    for i, noise_level in enumerate(noise_levels):
        pred = brain_3parts.copy().astype(int)

        # Add realistic segmentation errors
        for part_id in [1, 2, 3]:
            part_mask = (brain_3parts == part_id)

            # Random noise
            noise = np.random.rand(*pred.shape) < noise_level

            # Type 1: Misclassify some voxels to adjacent parts
            if part_id < 3:
                misclass_mask = part_mask & noise
                pred[misclass_mask] = part_id + 1

            # Type 2: Some voxels lost to background
            lost_mask = part_mask & (np.random.rand(*pred.shape) < noise_level/2)
            pred[lost_mask] = 0

            # Type 3: Some background becomes this part
            bg_mask = (brain_3parts == 0) & noise & (np.random.rand(*pred.shape) < noise_level/3)
            pred[bg_mask] = part_id

        predictions[f'SAM_Method_{i+1}'] = pred

    return predictions

# Create simulated predictions for 3-part segmentation
if brain_3parts is not None:
    print("üé≤ Creating simulated 3-part predictions...")
    sim_predictions_3part = create_simulated_3part_predictions(brain_3parts)

    # Calculate Dice scores for each part
    print("üìà Calculating Dice scores for 3-part segmentation...")

    dice_results_3part = {}
    for method_name, pred in sim_predictions_3part.items():
        dice_results_3part[method_name] = {}

        for part_id in [1, 2, 3]:
            true_mask = (brain_3parts == part_id)
            pred_mask = (pred == part_id)

            dice_score = calculate_dice_score(pred_mask, true_mask)
            dice_results_3part[method_name][part_names[part_id]] = dice_score

            print(f"   {method_name} - {part_names[part_id]}: Dice = {dice_score:.3f}")

# ============================================================================
# STEP 4: Extract 3D Surfaces for Each Part
# ============================================================================

def extract_3part_surfaces(brain_3parts, part_names, spacing):
    """Extract 3D surfaces for each of the 3 brain parts"""

    print("üé® Extracting 3D surfaces for each brain part...")

    surfaces = {}

    # Define colors for 3D visualization
    part_colors = {
        1: {'color': 'gray', 'opacity': 0.7},      # Grey Matter
        2: {'color': 'white', 'opacity': 0.8},     # White Matter
        3: {'color': 'cyan', 'opacity': 0.6}       # CSF
    }

    for part_id in [1, 2, 3]:
        if part_id not in part_names:
            continue

        print(f"   Extracting surface for {part_names[part_id]}...")

        # Extract mask for this part
        mask = (brain_3parts == part_id).astype(np.uint8)

        if not np.any(mask):
            print(f"   ‚ö†Ô∏è No voxels found for part {part_id}")
            continue

        # Smooth the mask
        mask = morphology.binary_closing(mask, morphology.ball(2))
        mask = morphology.binary_opening(mask, morphology.ball(1))
        mask_smooth = ndimage.gaussian_filter(mask.astype(float), sigma=1.5)

        try:
            # Extract surface
            vertices, faces, normals, values = measure.marching_cubes(
                mask_smooth,
                level=0.5,
                spacing=spacing,
                allow_degenerate=False
            )

            # Calculate properties
            volume_cm3 = np.sum(mask) * np.prod(spacing) / 1000
            surface_area_cm2 = measure.mesh_surface_area(vertices, faces) / 100

            surfaces[part_id] = {
                'vertices': vertices,
                'faces': faces,
                'normals': normals,
                'name': part_names[part_id],
                'color': part_colors[part_id]['color'],
                'opacity': part_colors[part_id]['opacity'],
                'volume_cm3': volume_cm3,
                'surface_area_cm2': surface_area_cm2
            }

            print(f"   ‚úÖ {part_names[part_id]}:")
            print(f"      Vertices: {len(vertices)}, Faces: {len(faces)}")
            print(f"      Volume: {volume_cm3:.1f} cm¬≥")
            print(f"      Surface Area: {surface_area_cm2:.1f} cm¬≤")

        except Exception as e:
            print(f"   ‚ùå Error extracting part {part_id}: {e}")
            continue

    return surfaces

# Extract 3D surfaces
if brain_3parts is not None:
    brain_3part_surfaces = extract_3part_surfaces(brain_3parts, part_names, spacing)

# ============================================================================
# STEP 5: Create Interactive 3D Visualization with 3 Parts
# ============================================================================

def create_3part_interactive_visualization(surfaces, save_dir):
    """Create interactive 3D visualization for the 3-part brain"""

    if not surfaces:
        print("‚ùå No surfaces to visualize")
        return None

    print("üé® Creating interactive 3-part brain visualization...")

    # Create figure
    fig = go.Figure()

    # Add each part as a separate trace
    for part_id, surface_data in surfaces.items():
        vertices = surface_data['vertices']
        faces = surface_data['faces']

        fig.add_trace(
            go.Mesh3d(
                x=vertices[:, 0],
                y=vertices[:, 1],
                z=vertices[:, 2],
                i=faces[:, 0],
                j=faces[:, 1],
                k=faces[:, 2],
                name=surface_data['name'],
                color=surface_data['color'],
                opacity=surface_data['opacity'],
                visible=True,
                lighting=dict(ambient=0.3, diffuse=0.8, specular=0.2),
                lightposition=dict(x=50, y=100, z=150)
            )
        )

    # Update layout
    fig.update_layout(
        title="Interactive 3-Part Brain Segmentation",
        scene=dict(
            aspectmode='data',
            camera=dict(
                eye=dict(x=1.8, y=1.8, z=1.8),
                center=dict(x=0, y=0, z=0)
            ),
            xaxis=dict(title='X (mm)', showbackground=True, backgroundcolor="rgb(230, 230, 250)"),
            yaxis=dict(title='Y (mm)', showbackground=True, backgroundcolor="rgb(250, 230, 230)"),
            zaxis=dict(title='Z (mm)', showbackground=True, backgroundcolor="rgb(230, 250, 230)")
        ),
        width=1000,
        height=800,
        margin=dict(r=20, b=10, l=10, t=40)
    )

    # Add control buttons
    part_buttons = []

    # Individual part buttons
    for i, (part_id, surface_data) in enumerate(surfaces.items()):
        visible = [False] * len(surfaces)
        visible[i] = True
        part_buttons.append(
            dict(
                label=f"Show {surface_data['name']}",
                method="update",
                args=[{"visible": visible}]
            )
        )

    # Combination buttons
    part_buttons.extend([
        dict(
            label="Show All 3 Brain Parts",
            method="update",
            args=[{"visible": [True] * len(surfaces)}]
        ),
        dict(
            label="Show Grey + White Matter",
            method="update",
            args=[{"visible": [True, True, False]}]
        ),
        dict(
            label="Hide All",
            method="update",
            args=[{"visible": [False] * len(surfaces)}]
        )
    ])

    fig.update_layout(
        updatemenus=[
            dict(
                type="dropdown",
                direction="down",
                showactive=True,
                x=0.02,
                y=1.0,
                buttons=part_buttons
            )
        ]
    )

    # Add annotation with part information
    annotation_text = "Brain Parts:<br>"
    for part_id, surface_data in surfaces.items():
        annotation_text += f"‚Ä¢ {surface_data['name']}: {surface_data['volume_cm3']:.1f} cm¬≥<br>"

    fig.add_annotation(
        x=0.02, y=0.98,
        xref="paper", yref="paper",
        text=annotation_text,
        showarrow=False,
        bgcolor="rgba(255,255,255,0.8)",
        bordercolor="black",
        borderwidth=1,
        font=dict(size=10)
    )

    # Show and save
    fig.show()
    fig.write_html(str(save_dir / 'interactive_3part_brain.html'))

    print(f"üíæ Interactive 3-part visualization saved!")
    return fig

# Create interactive visualization
if brain_3parts is not None and brain_3part_surfaces:
    interactive_3part_fig = create_3part_interactive_visualization(
        brain_3part_surfaces, directories['visualizations']
    )

# ============================================================================
# STEP 6: Save 3-Part Results
# ============================================================================

if brain_3parts is not None:
    # Save the 3-part segmentation
    np.savez_compressed(
        directories['results'] / 'brain_3part_segmentation.npz',
        brain_3parts=brain_3parts,
        original_image=sample_image,
        part_volumes=list(part_volumes.values()),
        spacing=spacing
    )

    # Save part names and results
    results_3part = {
        'part_names': part_names,
        'part_volumes_cm3': {str(k): float(v) for k, v in part_volumes.items()},
        'dice_scores': dice_results_3part if 'dice_results_3part' in locals() else {},
        'total_brain_volume_cm3': float(sum(part_volumes[i] for i in [1, 2, 3])),
        'surface_data': {
            str(part_id): {
                'volume_cm3': float(surf_data['volume_cm3']),
                'surface_area_cm2': float(surf_data['surface_area_cm2']),
                'num_vertices': len(surf_data['vertices']),
                'num_faces': len(surf_data['faces'])
            } for part_id, surf_data in brain_3part_surfaces.items()
        } if 'brain_3part_surfaces' in locals() else {}
    }

    with open(directories['results'] / '3part_brain_results.json', 'w') as f:
        json.dump(results_3part, f, indent=2)

print(f"\nüéâ Part 4 Complete - 3-Part Brain Segmentation!")
print(f"‚úÖ Brain divided into 3 distinct tissue types:")
if brain_3parts is not None:
    for part_id in [1, 2, 3]:
        print(f"   Part {part_id}: {part_names[part_id]} - {part_volumes[part_id]:.1f} cm¬≥")
print(f"‚úÖ Interactive 3D visualization with color-coded parts created")
print(f"‚úÖ Dice scores calculated for each part separately")
print(f"‚úÖ 3D surfaces extracted with different colors")
print(f"\nüéØ Next: Run Part 5 - Interactive Control Panel for Color/Opacity Control")

# Part 5: Interactive Control Panel with Color/Opacity Controls
# Run after Part 4

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import ipywidgets as widgets
from IPython.display import display, HTML
import plotly.colors as pc
from plotly.offline import iplot

print("üöÄ Part 5: Interactive Control Panel for 3D Brain Visualization")
print("="*60)

# Load data from previous steps
directories = {k: Path(v) for k, v in json.load(open('/content/brain_segmentation/paths.json'))['directories'].items()}

# Load 3-part segmentation data
seg_data_path = directories['results'] / 'brain_3part_segmentation.npz'
if seg_data_path.exists():
    seg_data = np.load(seg_data_path)
    brain_3parts = seg_data['brain_3parts']
    spacing = seg_data['spacing']
    print("‚úÖ Loaded 3-part brain segmentation data")
else:
    print("‚ùå 3-part segmentation data not found. Run Part 4 first!")
    brain_3parts = spacing = None

# Load results
results_path = directories['results'] / '3part_brain_results.json'
if results_path.exists():
    with open(results_path, 'r') as f:
        results_3part = json.load(f)
    part_names = results_3part['part_names']
    part_volumes = results_3part['part_volumes_cm3']
    print("‚úÖ Loaded 3-part results")
else:
    print("‚ùå Results not found. Run Part 4 first!")
    part_names = part_volumes = None

# ============================================================================
# STEP 1: Enhanced 3D Surface Extraction with Better Quality
# ============================================================================

def extract_high_quality_surfaces(brain_3parts, spacing, smoothing_iterations=3):
    """Extract high-quality 3D surfaces for interactive visualization"""

    print("üé® Extracting high-quality 3D surfaces...")

    from skimage import measure, morphology
    from scipy import ndimage

    surfaces = {}

    # Default color scheme
    default_colors = {
        1: {'color': '#808080', 'name': 'Grey Matter'},     # Gray
        2: {'color': '#FFFFFF', 'name': 'White Matter'},    # White
        3: {'color': '#00BFFF', 'name': 'CSF'}              # Deep Sky Blue
    }

    for part_id in [1, 2, 3]:
        if not np.any(brain_3parts == part_id):
            continue

        print(f"   Processing Part {part_id}: {default_colors[part_id]['name']}...")

        # Extract mask
        mask = (brain_3parts == part_id).astype(np.uint8)

        # Enhanced smoothing
        mask = morphology.binary_closing(mask, morphology.ball(3))
        mask = morphology.binary_opening(mask, morphology.ball(2))

        # Multiple Gaussian smoothing iterations
        mask_smooth = mask.astype(float)
        for _ in range(smoothing_iterations):
            mask_smooth = ndimage.gaussian_filter(mask_smooth, sigma=1.0)

        try:
            # Extract surface with higher resolution
            vertices, faces, normals, values = measure.marching_cubes(
                mask_smooth,
                level=0.3,
                spacing=spacing,
                allow_degenerate=False,
                step_size=1
            )

            # Simplify mesh if too dense
            if len(faces) > 50000:
                step = len(faces) // 30000
                faces = faces[::step]

            # Calculate properties
            volume_cm3 = np.sum(mask) * np.prod(spacing) / 1000
            surface_area_cm2 = measure.mesh_surface_area(vertices, faces) / 100

            surfaces[part_id] = {
                'vertices': vertices,
                'faces': faces,
                'normals': normals,
                'name': default_colors[part_id]['name'],
                'color': default_colors[part_id]['color'],
                'opacity': 0.8,
                'visible': True,
                'volume_cm3': volume_cm3,
                'surface_area_cm2': surface_area_cm2
            }

            print(f"      ‚úÖ {len(vertices)} vertices, {len(faces)} faces")

        except Exception as e:
            print(f"      ‚ùå Error: {e}")
            continue

    return surfaces

# Extract high-quality surfaces
if brain_3parts is not None:
    hq_surfaces = extract_high_quality_surfaces(brain_3parts, spacing)
else:
    hq_surfaces = {}

# ============================================================================
# STEP 2: Interactive Control Panel Class
# ============================================================================

class BrainVisualizationController:
    def __init__(self, surfaces):
        self.surfaces = surfaces
        self.fig = None
        self.current_colors = {}
        self.current_opacities = {}
        self.current_visibility = {}

        # Initialize current settings
        for part_id, surface in surfaces.items():
            self.current_colors[part_id] = surface['color']
            self.current_opacities[part_id] = surface['opacity']
            self.current_visibility[part_id] = surface['visible']

        self.setup_widgets()
        self.create_initial_plot()

    def setup_widgets(self):
        """Create interactive widgets for controlling visualization"""

        print("üéõÔ∏è Setting up interactive control panel...")

        # Color pickers for each part
        self.color_pickers = {}
        self.opacity_sliders = {}
        self.visibility_checkboxes = {}

        # Predefined color options
        color_options = [
            '#808080', '#FFFFFF', '#00BFFF', '#FFB6C1',  # Original colors
            '#FF8800', '#8800FF', '#00FFFF', '#FF00FF',  # Vibrant colors
            '#994444', '#449944', '#444499', '#999944',  # Darker colors
            '#CC6666', '#66CC66', '#6666CC', '#CCCC66'   # Softer colors
        ]

        control_widgets = []

        for part_id, surface in self.surfaces.items():
            part_name = surface['name']

            # Title for this part
            title = widgets.HTML(f"<h4 style='color: {surface['color']};'>üß† {part_name}</h4>")

            # Color picker
            color_picker = widgets.Dropdown(
                options=[(f'Color {i+1}', color) for i, color in enumerate(color_options)],
                value=surface['color'],
                description='Color:',
                style={'description_width': '60px'},
                layout=widgets.Layout(width='200px')
            )
            color_picker.part_id = part_id
            color_picker.observe(self.on_color_change, names='value')
            self.color_pickers[part_id] = color_picker

            # Opacity slider
            opacity_slider = widgets.FloatSlider(
                value=surface['opacity'],
                min=0.0,
                max=1.0,
                step=0.1,
                description='Opacity:',
                style={'description_width': '60px'},
                layout=widgets.Layout(width='300px')
            )
            opacity_slider.part_id = part_id
            opacity_slider.observe(self.on_opacity_change, names='value')
            self.opacity_sliders[part_id] = opacity_slider

            # Visibility checkbox
            visibility_checkbox = widgets.Checkbox(
                value=surface['visible'],
                description='Visible',
                style={'description_width': '60px'},
                layout=widgets.Layout(width='100px')
            )
            visibility_checkbox.part_id = part_id
            visibility_checkbox.observe(self.on_visibility_change, names='value')
            self.visibility_checkboxes[part_id] = visibility_checkbox

            # Volume info
            volume_info = widgets.HTML(
                f"<small>Volume: {surface['volume_cm3']:.1f} cm¬≥</small>"
            )

            # Group controls for this part
            part_controls = widgets.VBox([
                title,
                widgets.HBox([color_picker, opacity_slider, visibility_checkbox]),
                volume_info,
                widgets.HTML("<hr>")
            ])

            control_widgets.append(part_controls)

        # Global controls
        global_title = widgets.HTML("<h3>üéÆ Global Controls</h3>")

        # Show/Hide all buttons
        show_all_btn = widgets.Button(
            description='Show All',
            button_style='success',
            layout=widgets.Layout(width='100px')
        )
        show_all_btn.on_click(self.show_all)

        hide_all_btn = widgets.Button(
            description='Hide All',
            button_style='danger',
            layout=widgets.Layout(width='100px')
        )
        hide_all_btn.on_click(self.hide_all)

        # Reset button
        reset_btn = widgets.Button(
            description='Reset All',
            button_style='warning',
            layout=widgets.Layout(width='100px')
        )
        reset_btn.on_click(self.reset_all)

        # Preset buttons
        preset_title = widgets.HTML("<h4>üé® Color Presets</h4>")

        preset1_btn = widgets.Button(description='Medical', button_style='info')
        preset1_btn.on_click(lambda b: self.apply_preset('medical'))

        preset2_btn = widgets.Button(description='Vibrant', button_style='info')
        preset2_btn.on_click(lambda b: self.apply_preset('vibrant'))

        preset3_btn = widgets.Button(description='Pastel', button_style='info')
        preset3_btn.on_click(lambda b: self.apply_preset('pastel'))

        # Global opacity slider
        global_opacity = widgets.FloatSlider(
            value=0.8,
            min=0.0,
            max=1.0,
            step=0.1,
            description='Global Opacity:',
            style={'description_width': '100px'},
            layout=widgets.Layout(width='300px')
        )
        global_opacity.observe(self.on_global_opacity_change, names='value')

        # Combine all widgets
        global_controls = widgets.VBox([
            global_title,
            widgets.HBox([show_all_btn, hide_all_btn, reset_btn]),
            preset_title,
            widgets.HBox([preset1_btn, preset2_btn, preset3_btn]),
            global_opacity,
            widgets.HTML("<hr><hr>")
        ])

        # Main control panel
        self.control_panel = widgets.VBox([
            widgets.HTML("<h2>üß† Brain Visualization Controller</h2>"),
            global_controls
        ] + control_widgets)

    def create_initial_plot(self):
        """Create the initial 3D plot"""

        self.fig = go.FigureWidget()

        # Add each surface
        for part_id, surface in self.surfaces.items():
            vertices = surface['vertices']
            faces = surface['faces']

            trace = go.Mesh3d(
                x=vertices[:, 0],
                y=vertices[:, 1],
                z=vertices[:, 2],
                i=faces[:, 0],
                j=faces[:, 1],
                k=faces[:, 2],
                name=surface['name'],
                color=surface['color'],
                opacity=surface['opacity'],
                visible=surface['visible'],
                lighting=dict(
                    ambient=0.4,
                    diffuse=0.7,
                    specular=0.3,
                    roughness=0.1
                ),
                lightposition=dict(x=50, y=100, z=150),
                hovertemplate=f"<b>{surface['name']}</b><br>" +
                             f"Volume: {surface['volume_cm3']:.1f} cm¬≥<br>" +
                             f"Surface Area: {surface['surface_area_cm2']:.1f} cm¬≤<br>" +
                             "<extra></extra>"
            )

            self.fig.add_trace(trace)

        # Update layout
        self.fig.update_layout(
            title="Interactive 3D Brain Visualization Controller",
            scene=dict(
                aspectmode='data',
                camera=dict(
                    eye=dict(x=2, y=2, z=2),
                    center=dict(x=0, y=0, z=0)
                ),
                xaxis=dict(title='X (mm)', showbackground=True),
                yaxis=dict(title='Y (mm)', showbackground=True),
                zaxis=dict(title='Z (mm)', showbackground=True)
            ),
            width=900,
            height=700,
            margin=dict(r=20, b=10, l=10, t=60)
        )

    def on_color_change(self, change):
        """Handle color picker changes"""
        part_id = change['owner'].part_id
        new_color = change['new']

        self.current_colors[part_id] = new_color

        # Update the plot
        trace_idx = list(self.surfaces.keys()).index(part_id)
        with self.fig.batch_update():
            self.fig.data[trace_idx].color = new_color

    def on_opacity_change(self, change):
        """Handle opacity slider changes"""
        part_id = change['owner'].part_id
        new_opacity = change['new']

        self.current_opacities[part_id] = new_opacity

        # Update the plot
        trace_idx = list(self.surfaces.keys()).index(part_id)
        with self.fig.batch_update():
            self.fig.data[trace_idx].opacity = new_opacity

    def on_visibility_change(self, change):
        """Handle visibility checkbox changes"""
        part_id = change['owner'].part_id
        new_visibility = change['new']

        self.current_visibility[part_id] = new_visibility

        # Update the plot
        trace_idx = list(self.surfaces.keys()).index(part_id)
        with self.fig.batch_update():
            self.fig.data[trace_idx].visible = new_visibility

    def on_global_opacity_change(self, change):
        """Handle global opacity changes"""
        new_opacity = change['new']

        # Update all opacities
        for part_id in self.surfaces.keys():
            self.current_opacities[part_id] = new_opacity
            self.opacity_sliders[part_id].value = new_opacity

            trace_idx = list(self.surfaces.keys()).index(part_id)
            with self.fig.batch_update():
                self.fig.data[trace_idx].opacity = new_opacity

    def show_all(self, button):
        """Show all parts"""
        for part_id in self.surfaces.keys():
            self.visibility_checkboxes[part_id].value = True

    def hide_all(self, button):
        """Hide all parts"""
        for part_id in self.surfaces.keys():
            self.visibility_checkboxes[part_id].value = False

    def reset_all(self, button):
        """Reset all settings to default"""
        for part_id, surface in self.surfaces.items():
            self.color_pickers[part_id].value = surface['color']
            self.opacity_sliders[part_id].value = surface['opacity']
            self.visibility_checkboxes[part_id].value = surface['visible']

    def apply_preset(self, preset_name):
        """Apply color presets"""

        presets = {
            'medical': {
                1: '#696969',  # Dim Gray
                2: '#F5F5F5',  # White Smoke
                3: '#4682B4'   # Steel Blue
            },
            'vibrant': {
                1: '#FF1493',  # Deep Pink
                2: '#00FF7F',  # Spring Green
                3: '#1E90FF'   # Dodger Blue
            },
            'pastel': {
                1: '#D3D3D3',  # Light Gray
                2: '#FFFAF0',  # Floral White
                3: '#B0E0E6'   # Powder Blue
            }
        }

        if preset_name in presets:
            for part_id, color in presets[preset_name].items():
                if part_id in self.surfaces:
                    self.color_pickers[part_id].value = color

    def display(self):
        """Display the controller and plot"""

        # Create layout with controller and plot side by side
        layout = widgets.HBox([
            self.control_panel,
            widgets.VBox([
                widgets.HTML("<h3>üé® Real-time 3D Visualization</h3>"),
                self.fig
            ])
        ])

        display(layout)

        # Instructions
        instructions = widgets.HTML("""
        <div style='background-color: #f0f8ff; padding: 15px; border-radius: 10px; margin: 10px;'>
        <h4>üéÆ How to Use the Interactive Controller:</h4>
        <ul>
        <li><b>Color:</b> Select different colors for each brain part from dropdown</li>
        <li><b>Opacity:</b> Adjust transparency (0 = transparent, 1 = opaque)</li>
        <li><b>Visibility:</b> Show/hide individual parts with checkboxes</li>
        <li><b>Global Controls:</b> Show all, hide all, or reset to defaults</li>
        <li><b>Presets:</b> Apply predefined color schemes</li>
        <li><b>3D Navigation:</b> Click and drag to rotate, scroll to zoom</li>
        </ul>
        </div>
        """)

        display(instructions)

# ============================================================================
# STEP 3: Advanced Visualization Options
# ============================================================================

def create_comparison_view(surfaces, save_dir):
    """Create side-by-side comparison views"""

    print("üîç Creating comparison visualization...")

    # Create subplots
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('All Parts', 'Individual Parts', 'Cross-Section View', 'Statistics'),
        specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}],
               [{'type': 'scatter3d'}, {'type': 'bar'}]],
        vertical_spacing=0.08,
        horizontal_spacing=0.05
    )

    # Plot 1: All parts together
    for part_id, surface in surfaces.items():
        vertices = surface['vertices']
        faces = surface['faces']

        fig.add_trace(
            go.Mesh3d(
                x=vertices[:, 0], y=vertices[:, 1], z=vertices[:, 2],
                i=faces[:, 0], j=faces[:, 1], k=faces[:, 2],
                name=surface['name'],
                color=surface['color'],
                opacity=0.7,
                showlegend=True
            ),
            row=1, col=1
        )

    # Plot 2: Individual parts
    for i, (part_id, surface) in enumerate(surfaces.items()):
        vertices = surface['vertices']
        faces = surface['faces']

        fig.add_trace(
            go.Mesh3d(
                x=vertices[:, 0], y=vertices[:, 1], z=vertices[:, 2],
                i=faces[:, 0], j=faces[:, 1], k=faces[:, 2],
                name=f"Solo {surface['name']}",
                color=surface['color'],
                opacity=0.9,
                visible=(i == 0),
                showlegend=False
            ),
            row=1, col=2
        )

    # Plot 3: Cross-section
    z_slice = brain_3parts.shape[2] // 2
    slice_data = brain_3parts[:, :, z_slice]

    for part_id, surface in surfaces.items():
        coords = np.where(slice_data == part_id)
        if len(coords[0]) > 0:
            sample_size = min(1000, len(coords[0]))
            indices = np.random.choice(len(coords[0]), sample_size, replace=False)

            fig.add_trace(
                go.Scatter3d(
                    x=coords[0][indices] * spacing[0],
                    y=coords[1][indices] * spacing[1],
                    z=np.full(sample_size, z_slice * spacing[2]),
                    mode='markers',
                    marker=dict(size=3, color=surface['color'], opacity=0.8),
                    name=f"Cross-section {surface['name']}",
                    showlegend=False
                ),
                row=2, col=1
            )

    # Plot 4: Volume statistics
    volumes = [surface['volume_cm3'] for surface in surfaces.values()]
    names = [surface['name'] for surface in surfaces.values()]
    colors_bar = [surface['color'] for surface in surfaces.values()]

    fig.add_trace(
        go.Bar(
            x=names,
            y=volumes,
            marker_color=colors_bar,
            name='Volume (cm¬≥)',
            showlegend=False,
            text=[f"{v:.1f} cm¬≥" for v in volumes],
            textposition='auto'
        ),
        row=2, col=2
    )

    # Update layout
    fig.update_layout(
        title="Comprehensive Brain Analysis Dashboard",
        height=800,
        scene=dict(aspectmode='data'),
        scene2=dict(aspectmode='data'),
        scene3=dict(aspectmode='data')
    )

    # Add buttons for cycling through individual parts
    part_buttons = []
    for i, (part_id, surface) in enumerate(surfaces.items()):
        visible_list = [True] * len(surfaces)
        visible_list.extend([False] * len(surfaces))
        visible_list[len(surfaces) + i] = True
        visible_list.extend([True] * (len(surfaces) * 2))

        part_buttons.append(
            dict(
                label=f"Show {surface['name']} Solo",
                method="update",
                args=[{"visible": visible_list}]
            )
        )

    fig.update_layout(
        updatemenus=[
            dict(
                type="dropdown",
                direction="down",
                showactive=True,
                x=0.7, y=1.0,
                buttons=part_buttons
            )
        ]
    )

    fig.show()
    fig.write_html(str(save_dir / 'brain_comparison_dashboard.html'))

    return fig

# ============================================================================
# STEP 4: Initialize and Display Interactive Controller
# ============================================================================

if hq_surfaces:
    print("\nüéõÔ∏è Initializing Interactive Control Panel...")

    # Create the controller
    brain_controller = BrainVisualizationController(hq_surfaces)

    # Display the interactive interface
    print("üé® Launching Interactive Visualization Controller...")
    brain_controller.display()

    # Create comparison dashboard
    comparison_fig = create_comparison_view(hq_surfaces, directories['visualizations'])

    # Save the controller setup
    controller_settings = {
        'initial_colors': {str(k): v['color'] for k, v in hq_surfaces.items()},
        'initial_opacities': {str(k): v['opacity'] for k, v in hq_surfaces.items()},
        'part_volumes': {str(k): float(v['volume_cm3']) for k, v in hq_surfaces.items()},
        'surface_areas': {str(k): float(v['surface_area_cm2']) for k, v in hq_surfaces.items()}
    }

    with open(directories['results'] / 'controller_settings.json', 'w') as f:
        json.dump(controller_settings, f, indent=2)

    print(f"üíæ Controller settings saved to: {directories['results'] / 'controller_settings.json'}")

    print(f"\nüéâ Part 5 Complete!")
    print(f"‚úÖ Interactive control panel launched with full functionality")
    print(f"‚úÖ Real-time color, opacity, and visibility controls")
    print(f"‚úÖ Multiple visualization presets available")
    print(f"‚úÖ Comparison dashboard created")
    print(f"üíæ All settings and dashboards saved to: {directories['visualizations']}")

    # Display usage tips
    display(widgets.HTML("""
    <div style='background-color: #e6ffe6; padding: 15px; border-radius: 10px; margin: 20px 0;'>
    <h3>üéØ Interactive Features Available:</h3>
    <ul>
    <li><b>Real-time Controls:</b> Change colors, opacity, and visibility instantly</li>
    <li><b>Color Presets:</b> Medical, Vibrant, and Pastel color schemes</li>
    <li><b>3D Navigation:</b> Rotate, zoom, and pan the 3D model</li>
    <li><b>Individual Part Analysis:</b> Focus on specific brain regions</li>
    <li><b>Volume Information:</b> Hover over parts to see detailed metrics</li>
    <li><b>Export Options:</b> All visualizations saved as HTML files</li>
    </ul>
    <p><b>üéÆ Try experimenting with different settings to explore your brain segmentation!</b></p>
    </div>
    """))

else:
    print("‚ùå No surface data available. Please run Part 4 first!")

print(f"\nüèÅ Complete Brain Segmentation Pipeline Finished!")
print(f"üìä Summary of all generated files:")
print(f"   - 3D Interactive Controller: interactive_3part_brain.html")
print(f"   - Comparison Dashboard: brain_comparison_dashboard.html")
print(f"   - All numerical results: 3part_brain_results.json")
print(f"   - Controller settings: controller_settings.json")